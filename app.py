# -*- coding: utf-8 -*-
"""Final Project_Predictive

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JFGIYGdSFxccI_HcybBxN5jzigHUebKw
"""

!pip install pandas numpy matplotlib seaborn scikit-learn scipy nltk xgboost imbalanced-learn joblib wordcloud tensorflow shap

#Approach 2
"""
Employee Promotion Prediction Analysis
======================================

This code performs a comprehensive analysis of employee data to predict promotions,
including data preprocessing, exploratory data analysis, feature engineering,
model selection, and evaluation. Both supervised and unsupervised learning techniques
are implemented to address the business question:

"What factors influence employee promotion decisions and can we predict which employees
are likely to be promoted?"

Author: [Your Name]
Date: April 24, 2025
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectFromModel
import warnings
from scipy import stats
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
import xgboost as xgb
from imblearn.over_sampling import SMOTE
import joblib
import matplotlib.ticker as mtick
from wordcloud import WordCloud
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import precision_recall_curve, average_precision_score
import shap
import os

# Suppress warnings
warnings.filterwarnings('ignore')

# Set up plotting style
plt.style.use('fivethirtyeight')
sns.set_palette("coolwarm")

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Read CSV files directly (for quick testing)
# df_train = pd.read_csv("/content/sample_data/train.csv")
# df_test = pd.read_csv("/content/sample_data/test.csv")

class EmployeePromotionAnalysis:
    """
    A comprehensive class to analyze employee promotion data and build predictive models.
    """

    def __init__(self, train_path='/content/sample_data/train.csv', test_path='/content/sample_data/test.csv'):
        """
        Initialize the analysis with paths to the train and test datasets.

        Parameters:
        -----------
        train_path : str
            Path to the training dataset
        test_path : str
            Path to the test dataset
        """
        self.train_path = train_path
        self.test_path = test_path
        self.train_data = None
        self.test_data = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_validation = None
        self.X_validation = None
        self.feature_names = None
        self.categorical_features = None
        self.numeric_features = None
        self.models = {}
        self.best_model = None
        self.preprocessor = None
        self.clusters = None
        self.cluster_model = None

    def load_data(self):
        """
        Load the datasets and perform initial inspection.
        """
        print("Loading datasets...")
        self.train_data = pd.read_csv(self.train_path)
        self.test_data = pd.read_csv(self.test_path)

        print(f"Train dataset shape: {self.train_data.shape}")
        print(f"Test dataset shape: {self.test_data.shape}")

        # Display basic information about the datasets
        print("\nTrain dataset info:")
        print(self.train_data.info())

        print("\nTest dataset info:")
        print(self.test_data.info())

        # Check for duplicates
        print(f"\nDuplicates in train data: {self.train_data.duplicated().sum()}")
        print(f"Duplicates in test data: {self.test_data.duplicated().sum()}")

        return self.train_data, self.test_data

    def explore_data(self):
        """
        Perform exploratory data analysis to gain insights.
        """
        print("\n===== Exploratory Data Analysis =====")

        # Check for missing values
        train_missing = self.train_data.isnull().sum()
        test_missing = self.test_data.isnull().sum()

        print("\nMissing values in train data:")
        print(train_missing[train_missing > 0])

        print("\nMissing values in test data:")
        print(test_missing[test_missing > 0])

        # Analyze target variable distribution
        promotion_dist = self.train_data['is_promoted'].value_counts(normalize=True) * 100
        print("\nTarget variable distribution:")
        print(f"Not Promoted: {promotion_dist[0]:.2f}%")
        print(f"Promoted: {promotion_dist[1]:.2f}%")

        # Create visualizations directory
        import os
        if not os.path.exists('visualizations'):
            os.makedirs('visualizations')

        # Visualize target distribution
        plt.figure(figsize=(10, 6))
        sns.countplot(x='is_promoted', data=self.train_data)
        plt.title('Distribution of Target Variable (is_promoted)')
        plt.xlabel('Promotion Status (0=Not Promoted, 1=Promoted)')
        plt.ylabel('Count')
        plt.savefig('visualizations/target_distribution.png', bbox_inches='tight')
        plt.close()

        # Categorical variables analysis
        self.categorical_features = ['department', 'region', 'education', 'gender', 'recruitment_channel']
        for col in self.categorical_features:
            plt.figure(figsize=(12, 6))
            promotion_by_cat = self.train_data.groupby(col)['is_promoted'].mean() * 100

            ax = promotion_by_cat.sort_values(ascending=False).plot(kind='bar', color='skyblue')
            plt.title(f'Promotion Rate by {col.capitalize()}')
            plt.ylabel('Promotion Rate (%)')
            plt.xlabel(col.capitalize())
            plt.grid(axis='y', linestyle='--', alpha=0.7)

            # Add percentage labels on bars
            for i, v in enumerate(promotion_by_cat.sort_values(ascending=False)):
                ax.text(i, v + 0.5, f'{v:.1f}%', ha='center')

            plt.savefig(f'visualizations/promotion_by_{col}.png', bbox_inches='tight')
            plt.close()

        # Numerical variables analysis
        self.numeric_features = ['no_of_trainings', 'age', 'previous_year_rating',
                            'length_of_service', 'awards_won?', 'avg_training_score']

        # Create correlation heatmap
        plt.figure(figsize=(12, 10))
        numeric_data = self.train_data[self.numeric_features + ['is_promoted']]
        corr = numeric_data.corr()
        mask = np.triu(np.ones_like(corr, dtype=bool))
        sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',
                    linewidths=1, linecolor='white')
        plt.title('Correlation Matrix of Numerical Features')
        plt.tight_layout()
        plt.savefig('visualizations/correlation_matrix.png', bbox_inches='tight')
        plt.close()

        # Boxplots for numeric features by promotion status
        for col in self.numeric_features:
            plt.figure(figsize=(10, 6))
            sns.boxplot(x='is_promoted', y=col, data=self.train_data)
            plt.title(f'{col.capitalize()} by Promotion Status')
            plt.xlabel('Promotion Status (0=Not Promoted, 1=Promoted)')
            plt.ylabel(col.capitalize())
            plt.savefig(f'visualizations/boxplot_{col}.png', bbox_inches='tight')
            plt.close()

        # Distribution of numerical features
        for col in self.numeric_features:
            plt.figure(figsize=(12, 6))

            plt.subplot(1, 2, 1)
            sns.histplot(self.train_data[col], kde=True)
            plt.title(f'Distribution of {col.capitalize()}')
            plt.xlabel(col.capitalize())
            plt.ylabel('Frequency')

            plt.subplot(1, 2, 2)
            sns.histplot(data=self.train_data, x=col, hue='is_promoted',
                         multiple='stack', palette=['blue', 'red'])
            plt.title(f'Distribution of {col.capitalize()} by Promotion Status')
            plt.xlabel(col.capitalize())
            plt.ylabel('Frequency')

            plt.tight_layout()
            plt.savefig(f'visualizations/distribution_{col}.png', bbox_inches='tight')
            plt.close()

        # Age vs Length of Service plot with promotion info
        plt.figure(figsize=(12, 8))
        sns.scatterplot(x='length_of_service', y='age', hue='is_promoted',
                       data=self.train_data, alpha=0.6, palette=['blue', 'red'])
        plt.title('Age vs Length of Service by Promotion Status')
        plt.xlabel('Length of Service (Years)')
        plt.ylabel('Age (Years)')
        plt.savefig('visualizations/age_vs_service.png', bbox_inches='tight')
        plt.close()

        # Training score vs previous year rating
        plt.figure(figsize=(12, 8))
        sns.scatterplot(x='previous_year_rating', y='avg_training_score',
                       hue='is_promoted', data=self.train_data, alpha=0.6,
                       palette=['blue', 'red'])
        plt.title('Training Score vs Previous Year Rating by Promotion Status')
        plt.xlabel('Previous Year Rating')
        plt.ylabel('Average Training Score')
        plt.savefig('visualizations/score_vs_rating.png', bbox_inches='tight')
        plt.close()

        print("EDA completed. Visualizations saved to 'visualizations' directory.")

        return self

    def preprocess_data(self):
        """
        Preprocess the data including handling missing values,
        encoding categorical variables, and scaling numeric features.
        """
        print("\n===== Data Preprocessing =====")

        # Define the preprocessing steps
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])

        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])

        # Combine preprocessing steps
        self.preprocessor = ColumnTransformer(transformers=[
            ('num', numeric_transformer, self.numeric_features),
            ('cat', categorical_transformer, self.categorical_features)
        ])

        # Prepare the data for modeling
        X = self.train_data.drop(['employee_id', 'is_promoted'], axis=1)
        y = self.train_data['is_promoted']

        # Split the data into training and validation sets
        self.X_train, self.X_validation, self.y_train, self.y_validation = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"Training set shape: {self.X_train.shape}")
        print(f"Validation set shape: {self.X_validation.shape}")

        # Fit and transform the training data
        print("Applying preprocessing transformations...")
        X_train_processed = self.preprocessor.fit_transform(self.X_train)

        # Get feature names after transformation
        numeric_features_out = self.numeric_features

        ohe = self.preprocessor.named_transformers_['cat'].named_steps['encoder']
        categorical_features_out = ohe.get_feature_names_out(self.categorical_features)

        self.feature_names = np.concatenate([numeric_features_out, categorical_features_out])

        print(f"Number of features after preprocessing: {len(self.feature_names)}")

        return self

    def feature_engineering(self):
        """
        Perform feature engineering to create new features that might
        improve model performance.
        """
        print("\n===== Feature Engineering =====")

        # Add interaction features
        print("Creating interaction features...")

        # Age to service ratio (career progression speed)
        self.train_data['career_speed'] = self.train_data['age'] / (self.train_data['length_of_service'] + 1)
        self.test_data['career_speed'] = self.test_data['age'] / (self.test_data['length_of_service'] + 1)

        # Training efficiency (score per training)
        self.train_data['training_efficiency'] = self.train_data['avg_training_score'] / (self.train_data['no_of_trainings'] + 1)
        self.test_data['training_efficiency'] = self.test_data['avg_training_score'] / (self.test_data['no_of_trainings'] + 1)

        # Performance consistency (if previous rating is high)
        self.train_data['high_performer'] = (self.train_data['previous_year_rating'] >= 4).astype(int)
        self.test_data['high_performer'] = (self.test_data['previous_year_rating'] >= 4).astype(int)

        # Training to service ratio
        self.train_data['training_to_service'] = self.train_data['no_of_trainings'] / (self.train_data['length_of_service'] + 1)
        self.test_data['training_to_service'] = self.test_data['no_of_trainings'] / (self.test_data['length_of_service'] + 1)

        # Award to service ratio
        self.train_data['award_to_service'] = self.train_data['awards_won?'] / (self.train_data['length_of_service'] + 1)
        self.test_data['award_to_service'] = self.test_data['awards_won?'] / (self.test_data['length_of_service'] + 1)

        # Experience brackets
        self.train_data['experience_bracket'] = pd.cut(
            self.train_data['length_of_service'],
            bins=[0, 2, 5, 10, 20, 100],
            labels=['Very Junior', 'Junior', 'Mid-level', 'Senior', 'Expert']
        )
        self.test_data['experience_bracket'] = pd.cut(
            self.test_data['length_of_service'],
            bins=[0, 2, 5, 10, 20, 100],
            labels=['Very Junior', 'Junior', 'Mid-level', 'Senior', 'Expert']
        )

        # Age brackets
        self.train_data['age_bracket'] = pd.cut(
            self.train_data['age'],
            bins=[0, 25, 35, 45, 60, 100],
            labels=['Early Career', 'Developing', 'Established', 'Senior', 'Veteran']
        )
        self.test_data['age_bracket'] = pd.cut(
            self.test_data['age'],
            bins=[0, 25, 35, 45, 60, 100],
            labels=['Early Career', 'Developing', 'Established', 'Senior', 'Veteran']
        )

        # Convert the new categorical features
        self.categorical_features.extend(['experience_bracket', 'age_bracket'])

        # Add the new numerical features
        new_numeric_features = ['career_speed', 'training_efficiency', 'high_performer',
                              'training_to_service', 'award_to_service']
        self.numeric_features.extend(new_numeric_features)

        print(f"New features created: {new_numeric_features + ['experience_bracket', 'age_bracket']}")
        print(f"Total features after engineering: {len(self.numeric_features) + len(self.categorical_features)}")

        # Update the data for modeling
        X = self.train_data.drop(['employee_id', 'is_promoted'], axis=1)
        y = self.train_data['is_promoted']

        # Split the data again with the new features
        self.X_train, self.X_validation, self.y_train, self.y_validation = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # Update the preprocessor with new features
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])

        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])

        # Combine preprocessing steps
        self.preprocessor = ColumnTransformer(transformers=[
            ('num', numeric_transformer, self.numeric_features),
            ('cat', categorical_transformer, self.categorical_features)
        ])

        return self

    def clustering_analysis(self):
        """
        Perform unsupervised learning through clustering to discover hidden patterns.
        """
        print("\n===== Unsupervised Learning: Clustering Analysis =====")

        # Preprocess the data for clustering
        print("Preprocessing data for clustering...")
        # Only use numeric columns for clustering to avoid errors
        numeric_cols = self.X_train.select_dtypes(include=['number']).columns.tolist()

        # Create a simple preprocessor for clustering that only uses numeric data
        from sklearn.preprocessing import StandardScaler
        from sklearn.impute import SimpleImputer
        from sklearn.pipeline import Pipeline

        cluster_preprocessor = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])

        try:
            # Apply preprocessing to numeric data only
            X_numeric_train = self.X_train[numeric_cols].copy()
            X_preprocessed = cluster_preprocessor.fit_transform(X_numeric_train)

            # Determine optimal number of clusters using elbow method
            print("Finding optimal number of clusters...")
            inertia = []
            k_range = range(2, 11)

            for k in k_range:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                kmeans.fit(X_preprocessed)
                inertia.append(kmeans.inertia_)

            # Plot elbow curve
            plt.figure(figsize=(10, 6))
            plt.plot(k_range, inertia, marker='o')
            plt.title('Elbow Method for Optimal k')
            plt.xlabel('Number of clusters')
            plt.ylabel('Inertia')
            plt.grid(True, alpha=0.3)
            plt.savefig('visualizations/elbow_curve.png', bbox_inches='tight')
            plt.close()

            # Choose optimal k (this is simplified, in practice would analyze the plot)
            optimal_k = 4  # Example, adjust based on actual elbow curve
            print(f"Optimal number of clusters identified: {optimal_k}")

            # Fit clustering model with optimal k
            self.cluster_model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
            self.clusters = self.cluster_model.fit_predict(X_preprocessed)

            # Add cluster labels to the original data
            train_with_clusters = self.X_train.copy()
            train_with_clusters['cluster'] = self.clusters
            train_with_clusters['is_promoted'] = self.y_train.values

            # Analyze clusters
            print("\nCluster distribution:")
            cluster_counts = pd.Series(self.clusters).value_counts().sort_index()
            for i, count in enumerate(cluster_counts):
                print(f"Cluster {i}: {count} samples ({count/len(self.clusters)*100:.2f}%)")

            # Calculate promotion rate by cluster
            cluster_promotion = train_with_clusters.groupby('cluster')['is_promoted'].mean() * 100
            print("\nPromotion rate by cluster:")
            for i, rate in enumerate(cluster_promotion):
                print(f"Cluster {i}: {rate:.2f}%")

            # Visualize promotion rate by cluster
            plt.figure(figsize=(10, 6))
            ax = cluster_promotion.plot(kind='bar', color='skyblue')
            plt.title('Promotion Rate by Cluster')
            plt.xlabel('Cluster')
            plt.ylabel('Promotion Rate (%)')
            plt.grid(axis='y', linestyle='--', alpha=0.7)

            # Add percentage labels on bars
            for i, v in enumerate(cluster_promotion):
                ax.text(i, v + 0.5, f'{v:.1f}%', ha='center')

            plt.savefig('visualizations/promotion_by_cluster.png', bbox_inches='tight')
            plt.close()

            # Analyze cluster characteristics - ONLY use numeric columns
            print("\nCluster characteristics (numeric features only):")
            numeric_features_for_analysis = [col for col in train_with_clusters.columns
                                             if col in numeric_cols or col == 'is_promoted']

            # Explicitly create a new DataFrame with only numeric columns to avoid issues
            numeric_train_with_clusters = train_with_clusters[numeric_features_for_analysis].copy()

            # Then do the groupby on this numeric-only DataFrame
            cluster_features = numeric_train_with_clusters.groupby('cluster').mean()
            print(cluster_features)

            # For categorical columns, print the most common value in each cluster
            categorical_cols = [col for col in self.X_train.columns if col not in numeric_cols]
            if categorical_cols:
                print("\nMost common values for categorical features by cluster:")
                for col in categorical_cols:
                    print(f"\n{col}:")
                    for cluster_id in range(optimal_k):
                        cluster_data = train_with_clusters[train_with_clusters['cluster'] == cluster_id]
                        most_common = cluster_data[col].value_counts().index[0]
                        count = cluster_data[col].value_counts().iloc[0]
                        percentage = (count / len(cluster_data)) * 100
                        print(f"Cluster {cluster_id}: {most_common} ({percentage:.1f}%)")

            # Use PCA to visualize clusters in 2D
            pca = PCA(n_components=2)
            X_pca = pca.fit_transform(X_preprocessed)

            # Create a DataFrame for plotting
            pca_df = pd.DataFrame({
                'PCA1': X_pca[:, 0],
                'PCA2': X_pca[:, 1],
                'Cluster': self.clusters
            })

            # Add promotion status to the PCA dataframe
            # Convert to list to avoid any potential issues with indexing
            pca_df['Promoted'] = self.y_train.values.tolist()

            # Plot clusters
            plt.figure(figsize=(12, 10))

            plt.subplot(2, 1, 1)
            sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=pca_df, palette='viridis', alpha=0.7)
            plt.title('PCA Visualization of Clusters')

            plt.subplot(2, 1, 2)
            sns.scatterplot(x='PCA1', y='PCA2', hue='Promoted', data=pca_df, palette=['blue', 'red'], alpha=0.7)
            plt.title('PCA Visualization with Promotion Status')

            plt.tight_layout()
            plt.savefig('visualizations/cluster_pca_visualization.png', bbox_inches='tight')
            plt.close()

            # Add cluster as a feature for supervised learning
            self.train_data['cluster'] = np.nan
            mask = self.train_data.index.isin(self.X_train.index)
            self.train_data.loc[mask, 'cluster'] = self.clusters

            # Fill in cluster for validation data
            try:
                # Extract numeric features from validation data
                numeric_cols_validation = [col for col in numeric_cols if col in self.X_validation.columns]
                X_val_numeric = self.X_validation[numeric_cols_validation].copy()
                X_val_processed = cluster_preprocessor.transform(X_val_numeric)
                val_clusters = self.cluster_model.predict(X_val_processed)
                self.train_data.loc[~mask, 'cluster'] = val_clusters
            except Exception as e:
                print(f"Warning: Could not predict clusters for validation data: {e}")
                print("Continuing without cluster predictions for validation data.")

            # Add cluster to test data
            try:
                # Extract numeric features from test data
                test_numeric_cols = [col for col in numeric_cols if col in self.test_data.columns]
                X_test_numeric = self.test_data[test_numeric_cols].copy()
                X_test_processed = cluster_preprocessor.transform(X_test_numeric)
                test_clusters = self.cluster_model.predict(X_test_processed)
                self.test_data['cluster'] = test_clusters
            except Exception as e:
                print(f"Warning: Could not predict clusters for test data: {e}")
                print("Continuing without cluster predictions for test data.")

            # Update features
            self.categorical_features.append('cluster')

            # Generate simplified cluster profile report with only numeric features
            numeric_profile_cols = [
                'age', 'length_of_service', 'no_of_trainings', 'previous_year_rating',
                'avg_training_score', 'awards_won?', 'is_promoted'
            ]

            # Only include columns that exist in the dataframe
            profile_cols = [col for col in numeric_profile_cols if col in numeric_train_with_clusters.columns]

            # Generate the profile with only numeric columns
            cluster_profiles = numeric_train_with_clusters.groupby('cluster')[profile_cols].mean()

            print("\nCluster profiles (key metrics):")
            print(cluster_profiles)

            # Create cluster profile visualization
            plt.figure(figsize=(15, 10))
            # Normalize the values for better visualization
            cluster_profiles_norm = (cluster_profiles - cluster_profiles.min()) / (cluster_profiles.max() - cluster_profiles.min())

            # Plot heatmap
            sns.heatmap(cluster_profiles_norm, annot=cluster_profiles.round(2), cmap='YlGnBu', linewidths=.5)
            plt.title('Cluster Profiles (Normalized Features)')
            plt.savefig('visualizations/cluster_profiles.png', bbox_inches='tight')
            plt.close()

        except Exception as e:
            print(f"Error in clustering analysis: {e}")
            print("Continuing without clustering. The 'cluster' feature will not be available.")
            self.clusters = None
            self.cluster_model = None

        print("Clustering analysis completed.")
        return self

        print("\nCluster profiles:")
        print(cluster_profiles)

        # Create cluster profile visualization
        plt.figure(figsize=(15, 10))
        cluster_profiles_norm = (cluster_profiles - cluster_profiles.min()) / (cluster_profiles.max() - cluster_profiles.min())

        # Plot heatmap
        sns.heatmap(cluster_profiles_norm, annot=cluster_profiles.round(2), cmap='YlGnBu', linewidths=.5)
        plt.title('Cluster Profiles (Normalized Features)')
        plt.savefig('visualizations/cluster_profiles.png', bbox_inches='tight')
        plt.close()

        print("Clustering analysis completed.")
        return self

    def train_models(self):
        """
        Train multiple supervised models and evaluate their performance.
        """
        print("\n===== Supervised Learning: Model Training =====")

        # Apply preprocessing to the training and validation data
        X_train_processed = self.preprocessor.fit_transform(self.X_train)
        X_val_processed = self.preprocessor.transform(self.X_validation)

        # Check class imbalance
        unique, counts = np.unique(self.y_train, return_counts=True)
        print(f"Class distribution in training set: {dict(zip(unique, counts))}")

        # Addressing class imbalance with SMOTE
        print("Applying SMOTE to address class imbalance...")
        smote = SMOTE(random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, self.y_train)

        unique_resampled, counts_resampled = np.unique(y_train_resampled, return_counts=True)
        print(f"Class distribution after SMOTE: {dict(zip(unique_resampled, counts_resampled))}")

        # Define models to train
        models = {
            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
            'Random Forest': RandomForestClassifier(random_state=42),
            'Gradient Boosting': GradientBoostingClassifier(random_state=42),
            'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
        }

        # Dictionary to store model performance
        model_performance = {}

        # Train and evaluate each model
        for name, model in models.items():
            print(f"\nTraining {name}...")
            model.fit(X_train_resampled, y_train_resampled)

            # Predict on validation set
            y_pred = model.predict(X_val_processed)
            y_pred_proba = model.predict_proba(X_val_processed)[:, 1]

            # Calculate metrics
            accuracy = model.score(X_val_processed, self.y_validation)
            roc_auc = roc_auc_score(self.y_validation, y_pred_proba)

            # Store model and its performance
            self.models[name] = model
            model_performance[name] = {
                'accuracy': accuracy,
                'roc_auc': roc_auc,
                'y_pred': y_pred,
                'y_pred_proba': y_pred_proba
            }

            print(f"{name} - Accuracy: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}")

            # Print detailed classification report
            print(classification_report(self.y_validation, y_pred))

            # Plot confusion matrix
            plt.figure(figsize=(8, 6))
            cm = confusion_matrix(self.y_validation, y_pred)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
            plt.title(f'Confusion Matrix - {name}')
            plt.xlabel('Predicted Label')
            plt.ylabel('True Label')
            plt.savefig(f'visualizations/confusion_matrix_{name.replace(" ", "_").lower()}.png', bbox_inches='tight')
            plt.close()

            # Plot ROC curve
            plt.figure(figsize=(8, 6))
            fpr, tpr, _ = roc_curve(self.y_validation, y_pred_proba)
            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.4f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {name}')
            plt.legend(loc='lower right')
            plt.grid(True, alpha=0.3)
            plt.savefig(f'visualizations/roc_curve_{name.replace(" ", "_").lower()}.png', bbox_inches='tight')
            plt.close()

        # Compare model performance
        print("\nModel comparison:")
        perf_df = pd.DataFrame({name: {'Accuracy': metrics['accuracy'], 'ROC AUC': metrics['roc_auc']}
                               for name, metrics in model_performance.items()}).T
        print(perf_df.sort_values('ROC AUC', ascending=False))

        # Plot model comparison
        plt.figure(figsize=(12, 6))
        perf_df.plot(kind='bar', ylim=(0.5, 1.0))
        plt.title('Model Performance Comparison')
        plt.ylabel('Score')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.savefig('visualizations/model_comparison.png', bbox_inches='tight')
        plt.close()

        # Select the best model based on ROC AUC
        best_model_name = perf_df.sort_values('ROC AUC', ascending=False).index[0]
        self.best_model = self.models[best_model_name]
        print(f"\nBest model: {best_model_name}")

        # Deep Learning Model (simple neural network)
        print("\nTraining Neural Network model...")

        # Define neural network architecture
        model = Sequential([
            Dense(64, activation='relu', input_shape=(X_train_processed.shape[1],)),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(16, activation='relu'),
            Dense(1, activation='sigmoid')
        ])

        # Compile the model
        model.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

        # Define early stopping
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )

        # Train the model
        history = model.fit(
            X_train_resampled, y_train_resampled,
            epochs=50,
            batch_size=32,
            validation_data=(X_val_processed, self.y_validation),
            callbacks=[early_stopping],
            verbose=1
        )

        # Evaluate neural network
        nn_val_loss, nn_val_acc = model.evaluate(X_val_processed, self.y_validation, verbose=0)
        y_pred_proba_nn = model.predict(X_val_processed, verbose=0).ravel()
        y_pred_nn = (y_pred_proba_nn > 0.5).astype(int)
        nn_roc_auc = roc_auc_score(self.y_validation, y_pred_proba_nn)

        print(f"Neural Network - Accuracy: {nn_val_acc:.4f}, ROC AUC: {nn_roc_auc:.4f}")
        print(classification_report(self.y_validation, y_pred_nn))

        # Add to model performance dict
        self.models['Neural Network'] = model
        model_performance['Neural Network'] = {
            'accuracy': nn_val_acc,
            'roc_auc': nn_roc_auc,
            'y_pred': y_pred_nn,
            'y_pred_proba': y_pred_proba_nn
        }

        # Plot neural network training history
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper right')

        plt.subplot(1, 2, 2)
        plt.plot(history.history['accuracy'])
        plt.plot(history.history['val_accuracy'])
        plt.title('Model Accuracy')
        plt.ylabel('Accuracy')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='lower right')

        plt.tight_layout()
        plt.savefig('visualizations/neural_network_history.png', bbox_inches='tight')
        plt.close()

        # Compare all models including neural network
        print("\nFinal model comparison:")
        final_perf_df = pd.DataFrame({name: {'Accuracy': metrics['accuracy'], 'ROC AUC': metrics['roc_auc']}
                                    for name, metrics in model_performance.items()}).T
        print(final_perf_df.sort_values('ROC AUC', ascending=False))

        # Update best model if neural network is better
        if nn_roc_auc > perf_df['ROC AUC'].max():
            self.best_model = model
            best_model_name = 'Neural Network'
            print(f"Best model updated: {best_model_name}")

        # Plot all models comparison
        plt.figure(figsize=(14, 6))
        final_perf_df.plot(kind='bar', ylim=(0.5, 1.0))
        plt.title('All Models Performance Comparison')
        plt.ylabel('Score')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.savefig('visualizations/all_models_comparison.png', bbox_inches='tight')
        plt.close()

        # Plot Precision-Recall curve for all models
        plt.figure(figsize=(10, 8))
        for name, metrics in model_performance.items():
            precision, recall, _ = precision_recall_curve(self.y_validation, metrics['y_pred_proba'])
            avg_precision = average_precision_score(self.y_validation, metrics['y_pred_proba'])
            plt.plot(recall, precision, lw=2, label=f'{name} (AP = {avg_precision:.4f})')

        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve for All Models')
        plt.legend(loc='lower left')
        plt.grid(True, alpha=0.3)
        plt.savefig('visualizations/precision_recall_curve.png', bbox_inches='tight')
        plt.close()

        return self

    def feature_importance(self):
        """
        Analyze feature importance from the best model
        """
        print("\n===== Feature Importance Analysis =====")

        # Get the best model (assuming it's not Neural Network)
        if isinstance(self.best_model, (RandomForestClassifier, GradientBoostingClassifier, xgb.XGBClassifier)):
            # For tree-based models
            if isinstance(self.best_model, xgb.XGBClassifier):
                # For XGBoost
                importance = self.best_model.feature_importances_
            else:
                # For sklearn models
                importance = self.best_model.feature_importances_

            # Try to get feature names safely
            try:
                # Get feature names after preprocessing
                if hasattr(self.preprocessor, 'get_feature_names_out'):
                    feature_names = self.preprocessor.get_feature_names_out()
                else:
                    # Safe approach - use generic feature names if we can't get the actual names
                    feature_names = [f"feature_{i}" for i in range(len(importance))]

                    # Only try this if categorical transformer is available
                    if hasattr(self.preprocessor, 'named_transformers_') and 'cat' in self.preprocessor.named_transformers_:
                        # Try to manually construct feature names
                        try:
                            ohe = self.preprocessor.named_transformers_['cat'].named_steps['encoder']
                            categorical_features_out = ohe.get_feature_names_out(self.categorical_features)
                            feature_names = np.concatenate([self.numeric_features, categorical_features_out])
                        except:
                            # If the above fails, stick with generic feature names
                            print("Could not get detailed feature names, using generic ones.")
            except:
                # If all else fails, use generic feature names
                feature_names = [f"feature_{i}" for i in range(len(importance))]
                print("Could not extract feature names, using generic identifiers.")

            # Make sure feature_names is the correct length
            if len(feature_names) != len(importance):
                print(f"Warning: Feature names length ({len(feature_names)}) doesn't match importance length ({len(importance)})")
                feature_names = [f"feature_{i}" for i in range(len(importance))]

            # Create DataFrame for feature importance
            feature_importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importance
            }).sort_values('Importance', ascending=False)

            print("Top 15 most important features:")
            print(feature_importance_df.head(15))

            # Plot feature importances
            plt.figure(figsize=(12, 10))
            top_features = feature_importance_df.head(20)
            sns.barplot(x='Importance', y='Feature', data=top_features)
            plt.title('Top 20 Feature Importances')
            plt.tight_layout()
            plt.savefig('visualizations/feature_importance.png', bbox_inches='tight')
            plt.close()

        elif isinstance(self.best_model, LogisticRegression):
            # For logistic regression
            importance = np.abs(self.best_model.coef_[0])

            # Get feature names
            if hasattr(self.preprocessor, 'get_feature_names_out'):
                feature_names = self.preprocessor.get_feature_names_out()
            else:
                # Manual construction of feature names
                ohe = self.preprocessor.named_transformers_['cat'].named_steps['encoder']
                categorical_features_out = ohe.get_feature_names_out(self.categorical_features)
                feature_names = np.concatenate([self.numeric_features, categorical_features_out])

            # Create DataFrame for feature importance
            feature_importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importance
            }).sort_values('Importance', ascending=False)

            print("Top 15 most important features:")
            print(feature_importance_df.head(15))

            # Plot feature importances
            plt.figure(figsize=(12, 10))
            top_features = feature_importance_df.head(20)
            sns.barplot(x='Importance', y='Feature', data=top_features)
            plt.title('Top 20 Feature Importances (Absolute Coefficients)')
            plt.tight_layout()
            plt.savefig('visualizations/feature_importance.png', bbox_inches='tight')
            plt.close()

        else:
            print("Feature importance analysis is not available for the Neural Network model.")
            # If best model is Neural Network, use another model for feature importance
            if 'Random Forest' in self.models:
                print("Using Random Forest model for feature importance analysis instead.")
                rf_model = self.models['Random Forest']
                importance = rf_model.feature_importances_

                # Get feature names
                if hasattr(self.preprocessor, 'get_feature_names_out'):
                    feature_names = self.preprocessor.get_feature_names_out()
                else:
                    # Manual construction of feature names
                    ohe = self.preprocessor.named_transformers_['cat'].named_steps['encoder']
                    categorical_features_out = ohe.get_feature_names_out(self.categorical_features)
                    feature_names = np.concatenate([self.numeric_features, categorical_features_out])

                # Create DataFrame for feature importance
                feature_importance_df = pd.DataFrame({
                    'Feature': feature_names,
                    'Importance': importance
                }).sort_values('Importance', ascending=False)

                print("Top 15 most important features (from Random Forest):")
                print(feature_importance_df.head(15))

                # Plot feature importances
                plt.figure(figsize=(12, 10))
                top_features = feature_importance_df.head(20)
                sns.barplot(x='Importance', y='Feature', data=top_features)
                plt.title('Top 20 Feature Importances (Random Forest)')
                plt.tight_layout()
                plt.savefig('visualizations/feature_importance_rf.png', bbox_inches='tight')
                plt.close()

        # SHAP values analysis for model interpretability
        print("\nCalculating SHAP values for model interpretability...")
        try:
            if isinstance(self.best_model, (RandomForestClassifier, GradientBoostingClassifier, xgb.XGBClassifier, LogisticRegression)):
                X_processed = self.preprocessor.transform(self.X_validation.head(100))  # Use a subset for SHAP

                # Calculate SHAP values
                explainer = shap.Explainer(self.best_model, X_processed)
                shap_values = explainer(X_processed)

                # Plot SHAP summary
                plt.figure(figsize=(12, 10))
                shap.summary_plot(shap_values, X_processed, feature_names=feature_names, show=False)
                plt.title('SHAP Feature Importance')
                plt.tight_layout()
                plt.savefig('visualizations/shap_summary.png', bbox_inches='tight')
                plt.close()

                # Plot detailed SHAP values for top features
                plt.figure(figsize=(12, 8))
                shap.summary_plot(shap_values.values, X_processed, feature_names=feature_names,
                                max_display=10, show=False)
                plt.title('SHAP Values for Top Features')
                plt.tight_layout()
                plt.savefig('visualizations/shap_top_features.png', bbox_inches='tight')
                plt.close()

                print("SHAP analysis completed.")
            else:
                print("SHAP analysis is not implemented for Neural Network model.")
        except Exception as e:
            print(f"Error in SHAP analysis: {e}")
            print("Continuing without SHAP analysis.")

        return self

    def predict_test_data(self):
        """
        Make predictions on the test data using the best model.
        """
        print("\n===== Predictions on Test Data =====")

        # Prepare test data
        X_test = self.test_data.drop('employee_id', axis=1)
        X_test_processed = self.preprocessor.transform(X_test)

        # Make predictions with the best model
        if isinstance(self.best_model, tf.keras.Sequential):
            y_pred_proba = self.best_model.predict(X_test_processed, verbose=0).ravel()
            y_pred = (y_pred_proba > 0.5).astype(int)
        else:
            y_pred_proba = self.best_model.predict_proba(X_test_processed)[:, 1]
            y_pred = self.best_model.predict(X_test_processed)

        # Create submission dataframe
        submission = pd.DataFrame({
            'employee_id': self.test_data['employee_id'],
            'is_promoted': y_pred,
            'promotion_probability': y_pred_proba
        })

        print(f"Predictions made for {len(submission)} employees in test data")
        print(f"Promotion rate in test predictions: {submission['is_promoted'].mean()*100:.2f}%")

        # Save the predictions
        submission.to_csv('predictions.csv', index=False)
        print("Predictions saved to 'predictions.csv'")

        # Distribution of prediction probabilities
        plt.figure(figsize=(10, 6))
        sns.histplot(submission['promotion_probability'], bins=50, kde=True)
        plt.title('Distribution of Promotion Probabilities')
        plt.xlabel('Promotion Probability')
        plt.ylabel('Count')
        plt.axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold (0.5)')
        plt.legend()
        plt.savefig('visualizations/prediction_distribution.png', bbox_inches='tight')
        plt.close()

        return submission

    def run_complete_analysis(self):
        """
        Execute the complete analysis pipeline.
        """
        print("Starting Employee Promotion Prediction Analysis...")

        # Create visualizations directory if it doesn't exist
        if not os.path.exists('visualizations'):
            os.makedirs('visualizations')
            print("Created 'visualizations' directory")

        # Execute each step in sequence
        self.load_data()
        self.explore_data()
        self.preprocess_data()
        self.feature_engineering()
        self.clustering_analysis()
        self.train_models()
        self.feature_importance()
        submission = self.predict_test_data()

        print("\n===== Analysis Complete =====")
        print("All visualizations saved to 'visualizations' directory")
        print("Predictions saved to 'predictions.csv'")

        # Save the best model for future use
        joblib.dump(self.best_model, 'best_model.pkl')
        joblib.dump(self.preprocessor, 'preprocessor.pkl')
        print("Best model and preprocessor saved for future use")

        return submission


# Execute the analysis
if __name__ == "__main__":
    # Create an instance of the analysis class
    promotion_analysis = EmployeePromotionAnalysis(
        train_path="/content/sample_data/train.csv",
        test_path="/content/sample_data/test.csv"
    )

    # Run the complete analysis
    results = promotion_analysis.run_complete_analysis()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Set style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("coolwarm")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12

def generate_synthetic_data(n_samples=200):
    """Generates synthetic data resembling the HR promotion dataset."""
    np.random.seed(42)
    data = {
        'employee_id': range(1, n_samples + 1),
        'department': np.random.choice(['Sales', 'Operations', 'Marketing', 'Analytics', 'R&D'], n_samples),
        'region': np.random.randint(1, 10, n_samples),
        'education': np.random.choice(['Bachelors', 'Masters', 'PHD'], n_samples, p=[0.6, 0.3, 0.1]),
        'gender': np.random.choice(['m', 'f'], n_samples),
        'recruitment_channel': np.random.choice(['sourcing', 'referred', 'other'], n_samples),
        'no_of_trainings': np.random.randint(1, 5, n_samples),
        'age': np.random.randint(25, 45, n_samples),
        'previous_year_rating': np.random.choice([1, 2, 3, 4, 5, np.nan], n_samples, p=[0.15, 0.15, 0.25, 0.20, 0.20, 0.05]),
        'length_of_service': np.random.randint(1, 15, n_samples),
        'awards_won?': np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),
        'avg_training_score': np.random.randint(40, 95, n_samples),
        'is_promoted': np.random.choice([0, 1], n_samples, p=[0.8, 0.2])
    }
    return pd.DataFrame(data)

def target_distribution(train_data):
    """Visualize the distribution of the target variable"""
    plt.figure(figsize=(10, 6))
    ax = sns.countplot(x='is_promoted', data=train_data, palette=['skyblue', 'salmon'])

    # Add percentage labels on bars
    total = len(train_data)
    for p in ax.patches:
        height = p.get_height()
        percentage = height / total * 100
        ax.annotate(f'{height}\n({percentage:.1f}%)',
                     (p.get_x() + p.get_width() / 2., height),
                     ha='center', va='bottom')

    plt.title('Distribution of Target Variable (is_promoted)', fontsize=16)
    plt.xlabel('Promotion Status (0=Not Promoted, 1=Promoted)', fontsize=14)
    plt.ylabel('Count', fontsize=14)
    plt.xticks([0, 1], ['Not Promoted', 'Promoted'])
    plt.tight_layout()
    plt.show()

    # Print statistics
    promotion_counts = train_data['is_promoted'].value_counts()
    promotion_ratio = train_data['is_promoted'].mean() * 100
    print(f"Not Promoted: {promotion_counts[0]} ({100-promotion_ratio:.2f}%)")
    print(f"Promoted: {promotion_counts[1]} ({promotion_ratio:.2f}%)")

def correlation_matrix(train_data):
    """Create a correlation matrix heatmap"""
    # Select only numeric columns
    numeric_data = train_data.select_dtypes(include=['number'])

    # Calculate correlations
    corr = numeric_data.corr()

    # Plot the heatmap
    plt.figure(figsize=(14, 10))
    mask = np.triu(np.ones_like(corr, dtype=bool))
    cmap = sns.diverging_palette(230, 20, as_cmap=True)

    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
                square=True, linewidths=.5, annot=True, fmt='.2f')

    plt.title('Correlation Matrix of Numerical Features', fontsize=16)
    plt.tight_layout()
    plt.show()

    # Print top correlations with target
    if 'is_promoted' in numeric_data.columns:
        target_corr = numeric_data.corr()['is_promoted'].drop('is_promoted')
        print("Top correlations with is_promoted:")
        print(target_corr.sort_values(ascending=False))

def age_vs_service(train_data):
    """Create a scatter plot of age vs length of service"""
    plt.figure(figsize=(12, 8))

    # Create scatter plot
    scatter = sns.scatterplot(
        x='length_of_service',
        y='age',
        hue='is_promoted',
        data=train_data,
        alpha=0.6,
        palette=['steelblue', 'coral'],
        s=70
    )

    # Add a regression line for each class
    sns.regplot(
        x='length_of_service',
        y='age',
        data=train_data[train_data['is_promoted'] == 0],
        scatter=False,
        ci=None,
        line_kws={"color": "navy", "lw": 2, "linestyle": "--"}
    )

    sns.regplot(
        x='length_of_service',
        y='age',
        data=train_data[train_data['is_promoted'] == 1],
        scatter=False,
        ci=None,
        line_kws={"color": "darkred", "lw": 2, "linestyle": "--"}
    )

    # Customize the plot
    plt.title('Age vs Length of Service by Promotion Status', fontsize=16)
    plt.xlabel('Length of Service (Years)', fontsize=14)
    plt.ylabel('Age (Years)', fontsize=14)
    plt.legend(title='Promotion Status', labels=['Not Promoted', 'Promoted'])

    # Add annotations
    avg_age_promoted = train_data[train_data['is_promoted'] == 1]['age'].mean()
    avg_service_promoted = train_data[train_data['is_promoted'] == 1]['length_of_service'].mean()

    plt.annotate(
        f'Avg for promoted employees:\nAge: {avg_age_promoted:.1f}\nService: {avg_service_promoted:.1f}',
        xy=(avg_service_promoted, avg_age_promoted),
        xytext=(avg_service_promoted + 2, avg_age_promoted + 2),
        arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2", color='darkred')
    )

    plt.tight_layout()
    plt.show()

    # Print key statistics
    print("Age statistics by promotion status:")
    print(train_data.groupby('is_promoted')['age'].agg(['mean', 'min', 'max']))

    print("\nLength of service statistics by promotion status:")
    print(train_data.groupby('is_promoted')['length_of_service'].agg(['mean', 'min', 'max']))

def elbow_plot(train_data, max_clusters=10):
    """Create an elbow plot for K-means clustering"""
    # Select only numeric columns for clustering
    numeric_data = train_data.select_dtypes(include=['number'])

    if 'employee_id' in numeric_data.columns:
        numeric_data = numeric_data.drop('employee_id', axis=1)

    if 'is_promoted' in numeric_data.columns:
        numeric_data = numeric_data.drop('is_promoted', axis=1)

    # Handle missing values using imputation
    imputer = SimpleImputer(strategy='mean')
    scaled_data_imputed = imputer.fit_transform(numeric_data)

    # Standardize the data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(scaled_data_imputed)

    # Calculate inertia for different k values
    inertia = []
    k_range = range(1, max_clusters+1)

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(scaled_data)
        inertia.append(kmeans.inertia_)

    # Plot the elbow curve
    plt.figure(figsize=(12, 6))
    plt.plot(k_range, inertia, marker='o', linestyle='-', color='blue', linewidth=2, markersize=10)
    plt.xlabel('Number of Clusters (k)', fontsize=14)
    plt.ylabel('Inertia', fontsize=14)
    plt.title('Elbow Method for Optimal k', fontsize=16)
    plt.xticks(k_range)
    plt.grid(True, linestyle='--', alpha=0.7)

    # Add annotations for potential elbow points
    # Find the point of maximum curvature (approximate elbow)
    deltas = np.diff(inertia)
    delta_deltas = np.diff(deltas)
    if len(delta_deltas) > 0:
        elbow_index = np.argmax(delta_deltas) + 2  # +2 because of double diff and 1-indexing
        plt.annotate(
            f'Potential elbow point: k={elbow_index}',
            xy=(elbow_index, inertia[elbow_index-1]),
            xytext=(elbow_index+1, inertia[elbow_index-1] + (max(inertia) - min(inertia))/10),
            arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2")
        )
        optimal_k = elbow_index
    else:
        optimal_k = 3  # Default if elbow can't be clearly identified

    plt.tight_layout()
    plt.show()

    # Print inertia values
    print("Inertia values by number of clusters:")
    for k, i in zip(k_range, inertia):
        print(f"k={k}: {i:.2f}")

    # Suggest optimal k
    print(f"\nSuggested optimal number of clusters based on elbow method: {optimal_k}")

    return optimal_k

def pca_clusters(train_data, n_clusters=4):
    """Visualize clusters using PCA for dimensionality reduction"""
    # Select only numeric columns for clustering
    numeric_data = train_data.select_dtypes(include=['number'])

    if 'employee_id' in numeric_data.columns:
        numeric_data = numeric_data.drop('employee_id', axis=1)

    # Save the target variable if it exists
    if 'is_promoted' in numeric_data.columns:
        target = numeric_data['is_promoted']
        numeric_data = numeric_data.drop('is_promoted', axis=1)
    else:
        target = None

    # Handle missing values using imputation
    imputer = SimpleImputer(strategy='mean')
    scaled_data_imputed = imputer.fit_transform(numeric_data)

    # Standardize the data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(scaled_data_imputed)

    # Apply PCA to reduce to 2 dimensions
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(scaled_data)

    # Apply K-means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(scaled_data)

    # Create a DataFrame for plotting
    pca_df = pd.DataFrame(
        data=pca_result,
        columns=['Principal Component 1', 'Principal Component 2']
    )
    pca_df['Cluster'] = clusters

    if target is not None:
        pca_df['is_promoted'] = target.values

    # Plot clusters
    plt.figure(figsize=(20, 10))

    # First subplot: Clusters
    plt.subplot(1, 2, 1)
    sns.scatterplot(
        x='Principal Component 1',
        y='Principal Component 2',
        hue='Cluster',
        data=pca_df,
        palette='viridis',
        alpha=0.8,
        s=80
    )

    # Add cluster centroids
    centroids_pca = pca.transform(kmeans.cluster_centers_)
    plt.scatter(
        centroids_pca[:, 0],
        centroids_pca[:, 1],
        marker='X',
        s=200,
        c='red',
        label='Centroids'
    )

    plt.title('PCA Visualization of Clusters', fontsize=16)
    plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=14)
    plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=14)
    plt.legend(title='Cluster')

    # Second subplot: Target variable
    if target is not None:
        plt.subplot(1, 2, 2)
        sns.scatterplot(
            x='Principal Component 1',
            y='Principal Component 2',
            hue='is_promoted',
            data=pca_df,
            palette=['steelblue', 'coral'],
            alpha=0.8,
            s=80
        )
        plt.title('PCA Visualization by Promotion Status', fontsize=16)
        plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=14)
        plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=14)
        plt.legend(title='Promoted', labels=['No', 'Yes'])

    plt.tight_layout()
    plt.show()

    # Calculate cluster statistics
    if target is not None:
        cluster_stats = pca_df.groupby('Cluster')['is_promoted'].agg(['mean', 'count'])
        cluster_stats['promotion_rate'] = cluster_stats['mean'] * 100

        print("Cluster statistics:")
        print(cluster_stats[['count', 'promotion_rate']].sort_values('promotion_rate', ascending=False))

        # Identify the cluster with highest promotion rate
        best_cluster = cluster_stats['promotion_rate'].idxmax()
        print(f"\nCluster {best_cluster} has the highest promotion rate: {cluster_stats.loc[best_cluster, 'promotion_rate']:.2f}%")

    return pca, pca.explained_variance_ratio_

def feature_importance(train_data=None, importances=None):
    """Visualize feature importance"""
    # If no importances provided, create dummy data
    if importances is None:
        # Create dummy feature importance data
        features = [
            'previous_year_rating', 'avg_training_score', 'length_of_service',
            'awards_won?', 'age', 'no_of_trainings', 'gender_m', 'dept_sales',
            'dept_operations', 'region_7', 'education_masters', 'gender_f',
            'topic_0_prob', 'dept_promotion_rate', 'tfidf_award'
        ]

        importance = [0.23, 0.18, 0.15, 0.12, 0.08, 0.07, 0.05, 0.04, 0.03, 0.02, 0.01, 0.01, 0.005, 0.004, 0.001]

        # Create DataFrame
        feature_importance_df = pd.DataFrame({
            'Feature': features,
            'Importance': importance
        }).sort_values('Importance', ascending=False)
    else:
        # Use provided importances
        feature_importance_df = pd.DataFrame({
            'Feature': importances.keys(),
            'Importance': importances.values()
        }).sort_values('Importance', ascending=False)

    # Plot feature importance
    plt.figure(figsize=(14, 10))

    # Create the plot
    ax = sns.barplot(
        x='Importance',
        y='Feature',
        data=feature_importance_df.head(15),
        palette='viridis'
    )

    # Add value labels to the bars
    for p in ax.patches:
        width = p.get_width()
        plt.text(width + 0.005,
                 p.get_y() + p.get_height()/2,
                 f'{width:.3f}',
                 ha='left',
                 va='center'
                 )

    # Customize the plot
    plt.title('Top 15 Feature Importances', fontsize=16)
    plt.xlabel('Importance', fontsize=14)
    plt.ylabel('Feature', fontsize=14)
    plt.xlim(0, max(feature_importance_df['Importance']) * 1.2)  # Add 20% space for labels
    plt.grid(True, axis='x', linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.show()

    # Print feature importance
    print("Top 15 features by importance:")
    print(feature_importance_df.head(15))

# Main function to run all visualizations
def run_all_visualizations():
    """Run all visualizations one by one using synthetic data"""
    # Generate synthetic data
    print("Generating synthetic data...")
    train_data = generate_synthetic_data(n_samples=200)

    # 1. Target Distribution
    print("\n1. Visualizing target distribution...")
    target_distribution(train_data)

    # 2. Correlation Matrix
    print("\n2. Creating correlation matrix...")
    correlation_matrix(train_data)

    # 3. Age vs Service
    print("\n3. Plotting age vs service length...")
    age_vs_service(train_data)

    # 4. Elbow Plot
    print("\n4. Creating elbow plot for clustering...")
    optimal_k = elbow_plot(train_data, max_clusters=8)

    # 5. PCA Clusters
    print(f"\n5. Visualizing clusters using PCA (k={optimal_k})...")
    pca_clusters(train_data, n_clusters=optimal_k)

    # 6. Feature Importance
    print("\n6. Showing feature importance...")
    feature_importance()

    print("\nAll visualizations complete!")

# Execute all visualizations when running this script
if __name__ == "__main__":
    run_all_visualizations()

# Modified visualization code with medium sizes
def generate_visualizations(self):
    """Create and save all visualizations"""
    if not os.path.exists('visualizations'):
        os.makedirs('visualizations')

    # Common style parameters
    plt.rcParams.update({
        'figure.figsize': (8, 5),  # Medium size
        'axes.titlesize': 12,
        'axes.labelsize': 10,
        'xtick.labelsize': 8,
        'ytick.labelsize': 8
    })

    # 1. Target Distribution
    plt.figure()
    sns.countplot(x='is_promoted', data=self.train_data)
    plt.title('Promotion Distribution (0=Not Promoted, 1=Promoted)', pad=12)
    plt.tight_layout()
    plt.savefig('visualizations/target_distribution.png', dpi=100)
    plt.close()

    # 2. Correlation Matrix
    plt.figure()
    numeric_cols = ['age', 'length_of_service', 'avg_training_score']
    sns.heatmap(self.train_data[numeric_cols].corr(), annot=True, fmt=".2f", cmap='coolwarm')
    plt.title('Feature Correlation Matrix', pad=12)
    plt.tight_layout()
    plt.savefig('visualizations/correlation_matrix.png', dpi=100)
    plt.close()

    # 3. Age vs Service Plot
    plt.figure()
    ax = sns.scatterplot(x='length_of_service', y='age',
                       hue='is_promoted', data=self.train_data,
                       palette={0:'#1f77b4', 1:'#ff7f0e'},  # Custom colors
                       alpha=0.7)
    plt.title('Age vs Service Duration by Promotion Status', pad=12)
    plt.xlabel('Years of Service', labelpad=8)
    plt.ylabel('Age', labelpad=8)
    plt.legend(title='Promoted', loc='upper right')
    plt.tight_layout()
    plt.savefig('visualizations/age_vs_service.png', dpi=100)
    plt.close()

    print("Visualizations generated in medium size!")

# Install required packages
!pip install nltk wordcloud
!pip install matplotlib seaborn

import nltk

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
import warnings
import joblib
import time

# Suppress warnings
warnings.filterwarnings('ignore')

class EnhancedHRTextAnalysis(FastHRTextAnalysis):
    """
    Enhanced version of FastHRTextAnalysis with additional features:
    - Predictive modeling
    - Advanced feature engineering
    - Model evaluation and visualization
    - Hyperparameter tuning
    - Production readiness
    """

    def __init__(self, train_path='/content/sample_data/train.csv',
                 test_path='/content/sample_data/test.csv',
                 model_dir='./models'):
        super().__init__(train_path, test_path)
        self.model_dir = model_dir
        self.best_model = None
        self.feature_importances = None
        self.numerical_features = None
        self.feature_columns = None
        self.models_evaluated = {}

    def extract_advanced_features(self):
        """
        Extract more sophisticated features beyond the basic NLP features
        """
        print("Extracting advanced features...")

        # Get base NLP features first
        nlp_features = self.extract_nlp_features(max_tfidf_features=15, n_topics=5)

        # Now enhance with additional features

        # 1. Add interaction terms between topics and departments
        print("Creating interaction features...")
        topic_cols = [col for col in nlp_features.columns if col.startswith('topic_') and col.endswith('_prob')]

        # Create employee dataframe with IDs for merging
        enhanced_features = pd.DataFrame({'employee_id': nlp_features['employee_id']})

        # Add interaction between dept_promotion_rate and topic probabilities
        for topic_col in topic_cols:
            interaction_col = f"dept_rate_x_{topic_col}"
            enhanced_features[interaction_col] = nlp_features['dept_promotion_rate'] * nlp_features[topic_col]

        # 2. Calculate topic diversity (entropy of topic distribution)
        print("Calculating topic diversity...")

        def entropy(probs):
            """Calculate entropy of topic distribution"""
            probs = np.array(probs)
            # Avoid log(0)
            probs = np.clip(probs, 1e-10, 1)
            return -np.sum(probs * np.log(probs))

        enhanced_features['topic_entropy'] = nlp_features[topic_cols].apply(
            lambda row: entropy(row), axis=1
        )

        # 3. Add numerical features from original dataset
        print("Adding numerical features...")
        self.numerical_features = [
            'no_of_trainings', 'age', 'previous_year_rating',
            'length_of_service', 'avg_training_score'
        ]

        # Check which numerical features exist in the dataset
        available_num_features = [f for f in self.numerical_features if f in self.combined_data.columns]

        # Add numerical features to enhanced features
        for feature in available_num_features:
            enhanced_features[feature] = enhanced_features['employee_id'].map(
                self.combined_data.set_index('employee_id')[feature]
            )

        # 4. Create age groups and tenure groups for more interpretable features
        if 'age' in enhanced_features.columns:
            enhanced_features['age_group'] = pd.cut(
                enhanced_features['age'],
                bins=[0, 25, 30, 35, 40, 45, 50, 100],
                labels=['<25', '25-30', '30-35', '35-40', '40-45', '45-50', '50+']
            ).astype(str)

            # One-hot encode age groups
            age_dummies = pd.get_dummies(enhanced_features['age_group'], prefix='age')
            enhanced_features = pd.concat([enhanced_features, age_dummies], axis=1)

        if 'length_of_service' in enhanced_features.columns:
            enhanced_features['tenure_group'] = pd.cut(
                enhanced_features['length_of_service'],
                bins=[-1, 1, 3, 5, 10, 15, 50],
                labels=['<1', '1-3', '3-5', '5-10', '10-15', '15+']
            ).astype(str)

            # One-hot encode tenure groups
            tenure_dummies = pd.get_dummies(enhanced_features['tenure_group'], prefix='tenure')
            enhanced_features = pd.concat([enhanced_features, tenure_dummies], axis=1)

        # 5. Combine with original NLP features
        print("Combining all features...")
        combined_features = pd.merge(nlp_features, enhanced_features, on='employee_id', how='left')

        # Fill NAs for any missing values
        combined_features = combined_features.fillna(0)

        # Print stats
        feature_count = combined_features.shape[1] - 1  # Subtract 1 for employee_id
        print(f"Created {feature_count} total features")

        # Store feature columns for future use
        self.feature_columns = [col for col in combined_features.columns if col != 'employee_id']

        return combined_features

    def prepare_train_test_data(self, features_df):
        """
        Prepare training and testing datasets from the features dataframe
        """
        print("Preparing train/test datasets...")

        # Identify training data
        train_features = features_df[features_df['employee_id'].isin(self.train_data['employee_id'])].copy()

        # Get target variable
        train_features['is_promoted'] = train_features['employee_id'].map(
            self.train_data.set_index('employee_id')['is_promoted']
        )

        # Split into features and target
        X = train_features.drop(['employee_id', 'is_promoted'], axis=1)
        y = train_features['is_promoted']

        # Split into training and validation sets
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # Get test data
        test_features = features_df[features_df['employee_id'].isin(self.test_data['employee_id'])].copy()
        X_test = test_features.drop(['employee_id'], axis=1)

        print(f"Train set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}")

        return X_train, X_val, y_train, y_val, X_test, test_features['employee_id']

    def train_model(self, X_train, y_train, X_val, y_val, model_type='rf'):
        """
        Train a predictive model on the training data

        Parameters:
        - model_type: Type of model to train ('rf' for Random Forest, 'gb' for Gradient Boosting,
                     'lr' for Logistic Regression, 'xgb' for XGBoost)
        """
        print(f"Training {model_type} model...")

        # Initialize model based on type
        if model_type == 'rf':
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=10,
                min_samples_leaf=5,
                random_state=42,
                n_jobs=-1
            )
            model_name = "Random Forest"
        elif model_type == 'gb':
            model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            )
            model_name = "Gradient Boosting"
        elif model_type == 'lr':
            model = LogisticRegression(
                C=1.0,
                max_iter=1000,
                random_state=42,
                n_jobs=-1
            )
            model_name = "Logistic Regression"
        elif model_type == 'xgb':
            model = XGBClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1
            )
            model_name = "XGBoost"
        else:
            raise ValueError(f"Unknown model type: {model_type}")

        # Train the model
        start_time = time.time()
        model.fit(X_train, y_train)
        training_time = time.time() - start_time

        # Evaluate on validation set
        y_pred = model.predict(X_val)
        y_prob = model.predict_proba(X_val)[:, 1]

        # Calculate metrics
        accuracy = accuracy_score(y_val, y_pred)
        precision = precision_score(y_val, y_pred)
        recall = recall_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, y_prob)

        # Store evaluation results
        self.models_evaluated[model_type] = {
            'model': model,
            'name': model_name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'training_time': training_time
        }

        # Print evaluation results
        print(f"\n{model_name} Model Evaluation:")
        print(f"Accuracy:  {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall:    {recall:.4f}")
        print(f"F1 Score:  {f1:.4f}")
        print(f"AUC:       {auc:.4f}")
        print(f"Training Time: {training_time:.2f} seconds")

        # Get feature importances for tree-based models
        if model_type in ['rf', 'gb', 'xgb']:
            if model_type == 'xgb':
                importances = model.feature_importances_
            else:
                importances = model.feature_importances_

            feature_importance = pd.DataFrame({
                'feature': X_train.columns,
                'importance': importances
            }).sort_values('importance', ascending=False)

            print("\nTop 10 Important Features:")
            for i, row in feature_importance.head(10).iterrows():
                print(f"{row['feature']}: {row['importance']:.4f}")

            # Store feature importances
            self.feature_importances = feature_importance

        return model

    def optimize_hyperparameters(self, X_train, y_train, model_type='rf'):
        """
        Perform hyperparameter optimization using GridSearchCV
        """
        print(f"Optimizing hyperparameters for {model_type} model...")

        # Define parameter grids for different model types
        if model_type == 'rf':
            model = RandomForestClassifier(random_state=42)
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [5, 10, 15, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            model_name = "Random Forest"
        elif model_type == 'gb':
            model = GradientBoostingClassifier(random_state=42)
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 5, 7],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 1.0]
            }
            model_name = "Gradient Boosting"
        elif model_type == 'xgb':
            model = XGBClassifier(random_state=42)
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 5, 7],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 1.0],
                'colsample_bytree': [0.8, 1.0]
            }
            model_name = "XGBoost"
        else:
            raise ValueError(f"Hyperparameter optimization not supported for model type: {model_type}")

        # Setup cross-validation
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

        # Run grid search (with smaller param grid for demonstration)
        grid_search = GridSearchCV(
            model, param_grid,
            cv=cv,
            scoring='f1',
            n_jobs=-1,
            verbose=1
        )

        # Fit grid search
        start_time = time.time()
        grid_search.fit(X_train, y_train)
        optimization_time = time.time() - start_time

        # Print results
        print(f"\nBest parameters for {model_name}:")
        for param, value in grid_search.best_params_.items():
            print(f"{param}: {value}")

        print(f"\nBest F1 score: {grid_search.best_score_:.4f}")
        print(f"Optimization time: {optimization_time:.2f} seconds")

        return grid_search.best_estimator_

    def visualize_results(self, X_val, y_val, model):
        """
        Create visualizations for model evaluation and interpretation
        """
        print("Generating visualizations...")

        # 1. ROC curve
        y_prob = model.predict_proba(X_val)[:, 1]
        fpr, tpr, _ = roc_curve(y_val, y_prob)

        plt.figure(figsize=(10, 6))
        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc_score(y_val, y_prob):.4f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC) Curve')
        plt.legend(loc='lower right')
        plt.grid(True)
        plt.savefig('roc_curve.png')
        plt.close()

        # 2. Feature importance plot (if available)
        if self.feature_importances is not None:
            plt.figure(figsize=(12, 8))
            top_features = self.feature_importances.head(15)
            sns.barplot(x='importance', y='feature', data=top_features)
            plt.title('Top 15 Feature Importances')
            plt.tight_layout()
            plt.savefig('feature_importance.png')
            plt.close()

        # 3. Confusion Matrix
        y_pred = model.predict(X_val)
        cm = confusion_matrix(y_val, y_pred)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title('Confusion Matrix')
        plt.savefig('confusion_matrix.png')
        plt.close()

        # 4. Model comparison (if multiple models evaluated)
        if len(self.models_evaluated) > 1:
            models_df = pd.DataFrame({
                'Model': [m['name'] for m in self.models_evaluated.values()],
                'Accuracy': [m['accuracy'] for m in self.models_evaluated.values()],
                'Precision': [m['precision'] for m in self.models_evaluated.values()],
                'Recall': [m['recall'] for m in self.models_evaluated.values()],
                'F1 Score': [m['f1'] for m in self.models_evaluated.values()],
                'AUC': [m['auc'] for m in self.models_evaluated.values()]
            })

            plt.figure(figsize=(14, 8))
            metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']
            models_df_melted = pd.melt(models_df, id_vars=['Model'], value_vars=metrics, var_name='Metric', value_name='Score')

            sns.barplot(x='Model', y='Score', hue='Metric', data=models_df_melted)
            plt.title('Model Comparison')
            plt.ylim(0, 1)
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig('model_comparison.png')
            plt.close()

        print("Visualizations saved to current directory")

    def predict_promotions(self, model, X_test, employee_ids):
        """
        Generate predictions for the test data
        """
        print("Generating promotion predictions...")

        # Predict probabilities
        y_prob = model.predict_proba(X_test)[:, 1]

        # Create submission dataframe
        predictions = pd.DataFrame({
            'employee_id': employee_ids,
            'promotion_probability': y_prob,
            'is_promoted': (y_prob > 0.5).astype(int)
        })

        # Save predictions
        predictions.to_csv('promotion_predictions.csv', index=False)

        # Show sample predictions
        print("\nSample predictions (first 5 rows):")
        print(predictions.head())

        return predictions

    def save_model(self, model, name='best_model'):
        """
        Save the trained model to disk
        """
        import os

        # Create model directory if it doesn't exist
        if not os.path.exists(self.model_dir):
            os.makedirs(self.model_dir)

        # Save model
        model_path = os.path.join(self.model_dir, f"{name}.joblib")
        joblib.dump(model, model_path)

        print(f"Model saved to {model_path}")

        return model_path

    def run_complete_analysis(self, train_models=True, optimize=False, visualize=True):
        """
        Run the complete enhanced analysis pipeline
        """
        print("Starting Enhanced HR Text Analysis...")

        # 1. Load data and create profiles
        self.load_data()
        self.create_employee_text_profiles()

        # 2. Extract enhanced features
        features_df = self.extract_advanced_features()

        # 3. Prepare training and test data
        X_train, X_val, y_train, y_val, X_test, employee_ids = self.prepare_train_test_data(features_df)

        # 4. Train and evaluate models (optional)
        if train_models:
            for model_type in ['rf', 'gb', 'xgb']:
                print(f"\n{'='*20} Training {model_type} model {'='*20}")

                # Optimize hyperparameters if requested
                if optimize:
                    model = self.optimize_hyperparameters(X_train, y_train, model_type)
                else:
                    model = self.train_model(X_train, y_train, X_val, y_val, model_type)

                # Set as best model if it's the first one or has better F1 score than current best
                if (self.best_model is None or
                    self.models_evaluated[model_type]['f1'] > self.models_evaluated[self.best_model]['f1']):
                    self.best_model = model_type

            # Display best model
            print(f"\nBest model: {self.models_evaluated[self.best_model]['name']}")
            print(f"F1 Score: {self.models_evaluated[self.best_model]['f1']:.4f}")

            # 5. Visualize results
            if visualize:
                self.visualize_results(X_val, y_val, self.models_evaluated[self.best_model]['model'])

            # 6. Generate predictions for test data
            predictions = self.predict_promotions(
                self.models_evaluated[self.best_model]['model'],
                X_test,
                employee_ids
            )

            # 7. Save the best model
            self.save_model(self.models_evaluated[self.best_model]['model'])

        print("\n===== Enhanced HR Text Analysis Complete =====")
        return features_df


# Example usage:
def enhanced_hr_analysis():
    """Run the enhanced HR analysis pipeline"""
    # Initialize enhanced analyzer
    hr_analyzer = EnhancedHRTextAnalysis(
        train_path="/content/sample_data/train.csv",
        test_path="/content/sample_data/test.csv",
        model_dir="./models"
    )

    # Run complete analysis with all optional steps
    enhanced_features = hr_analyzer.run_complete_analysis(
        train_models=True,
        optimize=False,  # Set to True for hyperparameter optimization (slower)
        visualize=True
    )

    print("\nEnhanced HR Analysis Complete!")
    print(f"Total features created: {enhanced_features.shape[1] - 1}")
    print(f"Total employees analyzed: {enhanced_features.shape[0]}")

    return hr_analyzer

# Run the enhanced analysis
# analyzer = enhanced_hr_analysis()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Ensures plots display in Google Colab
# %matplotlib inline

# Function to plot department-wise promotion rates
def department_promotion_rates(data):
    # Grouping by department and calculating promotion rate
    dept_promotions = data.groupby('department')['is_promoted'].mean().sort_values(ascending=False)

    # Plotting
    plt.figure(figsize=(10, 6))
    sns.barplot(x=dept_promotions.index, y=dept_promotions.values, color='salmon')
    plt.xticks(rotation=45, ha='right')
    plt.title('Promotion Rate by Department')
    plt.ylabel('Promotion Rate')
    plt.xlabel('Department')
    plt.tight_layout()
    plt.show()

# Call the function with your dataset
department_promotion_rates(train_data)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Ensures plots display in Google Colab
# %matplotlib inline

# Function to plot feature distributions by promotion status
def feature_distributions_by_promotion_status(data):
    # Selecting numerical features except the target column
    numeric_cols = data.select_dtypes(include='number').columns
    numeric_cols = [col for col in numeric_cols if col != 'is_promoted']

    # Loop through each numerical column
    for col in numeric_cols:
        plt.figure(figsize=(10, 6))
        sns.histplot(data=data, x=col, hue='is_promoted', element='step',
                     palette={0: 'salmon', 1: 'teal'}, bins=30, kde=True)
        plt.title(f'Distribution of {col} by Promotion Status')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.tight_layout()
        plt.show()

# Call the function with your dataset
feature_distributions_by_promotion_status(train_data)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (roc_curve, auc, confusion_matrix,
                             precision_recall_curve, average_precision_score)
import numpy as np

def plot_evaluation_metrics(model, X_test, y_test):
    """
    Generate three key evaluation visualizations for classification models

    Parameters:
    model: Trained classifier
    X_test: Test features
    y_test: True labels
    """
    plt.rcParams['figure.figsize'] = (10, 4)
    plt.rcParams['font.size'] = 10

    # Generate predictions and probabilities
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    # 1. ROC Curve
    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'XGBoost (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Sensitivity vs Specificity Trade-off')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    plt.show()

    # 2. Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)

    plt.figure()
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                cbar=False, annot_kws={'size':14})
    plt.title('Confusion Matrix - Prediction Breakdown')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.xticks([0.5,1.5], ['Not Promoted', 'Promoted'])
    plt.yticks([0.5,1.5], ['Not Promoted', 'Promoted'], va='center')
    plt.show()

    # 3. Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(y_test, y_proba)
    avg_precision = average_precision_score(y_test, y_proba)

    plt.figure()
    plt.plot(recall, precision, color='darkgreen', lw=2,
            label=f'XGBoost (AP = {avg_precision:.2f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve - Imbalanced Class Performance')
    plt.legend(loc='best')
    plt.grid(alpha=0.3)

    # Add baseline for imbalanced classes
    baseline = len(y_test[y_test==1]) / len(y_test)
    plt.axhline(y=baseline, color='gray', linestyle='--',
               label='Random Classifier')
    plt.legend()
    plt.show()

# Example usage:
if __name__ == "__main__":
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from xgboost import XGBClassifier

    # Generate synthetic imbalanced data
    X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1], random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train model
    model = XGBClassifier()
    model.fit(X_train, y_train)

    # Generate visualizations
    plot_evaluation_metrics(model, X_test, y_test)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import shap

# Set Colab-friendly style
plt.rcParams['figure.figsize'] = (12, 6)
sns.set_style("whitegrid")
SALMON_PALETTE = ["#FF8C7F", "#FA8072", "#E9967A", "#F08080"]

def plot_feature_importance(model, feature_names):
    """Plot feature importance with salmon pink styling"""
    # Get importance scores
    importance = model.feature_importances_
    indices = np.argsort(importance)[-15:]  # Top 15 features

    # Create DataFrame
    feat_imp = pd.DataFrame({
        'feature': feature_names[indices],
        'importance': importance[indices]
    }).sort_values('importance', ascending=True)

    # Plotting
    plt.figure(figsize=(10, 8))
    bar = sns.barplot(
        x='importance',
        y='feature',
        data=feat_imp,
        palette=SALMON_PALETTE*4,
        edgecolor='w',
        linewidth=1
    )

    # Add annotations
    for i, v in enumerate(feat_imp.importance):
        bar.text(v + 0.005, i, f"{v:.3f}",
                color='dimgray', va='center', fontsize=10)

    plt.title("Top 15 Feature Importances\nPerformance-Driven Promotion Factors",
             fontsize=14, pad=20)
    plt.xlabel("Importance Score", fontsize=12)
    plt.ylabel("")
    plt.xlim(0, feat_imp.importance.max()*1.15)

    # Add explanatory text
    textstr = """Key Insights:
- Previous year rating: Strongest predictor (0.230)
- Training scores: Critical for readiness (0.180)
- Service length: Org knowledge indicator (0.150)
- Awards won: Performance validation (0.120)"""
    plt.gcf().text(0.72, 0.65, textstr, fontsize=11,
                 bbox=dict(facecolor='white', alpha=0.8))

    sns.despine()
    plt.tight_layout()
    plt.show()

def plot_shap_values(model, X):
    """SHAP value visualization with custom styling"""
    # Initialize SHAP
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)

    # Plot configuration
    plt.figure(figsize=(12, 6))
    shap.summary_plot(
        shap_values,
        X,
        plot_type="bar",
        color_bar=False,
        max_display=15,
        show=False
    )

    # Styling adjustments
    plt.gcf().axes[0].set_xlabel("Mean |SHAP Value|", fontsize=12)
    plt.title("SHAP Feature Impact Analysis", fontsize=14, pad=20)
    plt.gca().set_facecolor('#FFF5F3')
    plt.grid(axis='x', alpha=0.3)

    # Color override for salmon pink
    for fc in plt.gcf().axes[0].get_children():
        if isinstance(fc, plt.Rectangle):
            fc.set_facecolor(SALMON_PALETTE[0])
            fc.set_edgecolor('w')

    plt.tight_layout()
    plt.show()

# Example usage:
if __name__ == "__main__":
    # Install dependencies
    !pip install shap

    # Sample data format (replace with your actual data)
    from sklearn.datasets import make_classification
    X, y = make_classification(n_features=20)
    feature_names = [f"Feature_{i}" for i in range(20)]

    # Train model
    from xgboost import XGBClassifier
    model = XGBClassifier().fit(X, y)

    # Generate visualizations
    plot_feature_importance(model, np.array(feature_names))
    plot_shap_values(model, X)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import shap
import pandas as pd

# Set improved styling
plt.rcParams['figure.figsize'] = (14, 8)
sns.set_style("whitegrid")
COLOR_PALETTE = ["#FF6B6B", "#FF8E8E", "#FFAAAA", "#FFC3C3", "#FFD8D8"]

# Meaningful feature names mapping (replace with your actual names)
FEATURE_NAMES = {
    0: "Age",
    1: "Department",
    2: "Education Level",
    3: "Gender",
    4: "Region",
    5: "Previous Year Rating",  # Most important feature
    6: "Number of Trainings",
    7: "KPIs Met",
    8: "Awards Won",
    9: "Average Training Score",
    10: "Length of Service",
    11: "Recruitment Channel",
    12: "Performance Bonus",
    13: "Peer Reviews",
    14: "Manager Approval",
    15: "Project Completion Rate",
    16: "Overtime Hours",
    17: "Salary Grade",
    18: "Team Size",
    19: "Tenure in Role"
}

def plot_feature_importance(model, feature_indices):
    """Enhanced feature importance plot with clear labeling"""
    # Get importance scores
    importance = model.feature_importances_
    indices = np.argsort(importance)[-15:]  # Top 15 features

    # Create named DataFrame
    feat_imp = pd.DataFrame({
        'feature': [FEATURE_NAMES[i] for i in feature_indices[indices]],
        'importance': importance[indices]
    }).sort_values('importance', ascending=True)

    # Create gradient color mapping
    colors = sns.color_palette(COLOR_PALETTE, n_colors=len(feat_imp))

    plt.figure(figsize=(12, 8))
    bar = sns.barplot(
        x='importance',
        y='feature',
        data=feat_imp,
        palette=colors,
        edgecolor='#2D2D2D',
        linewidth=0.5
    )

    # Improved annotations
    for i, v in enumerate(feat_imp.importance):
        bar.text(v + 0.005, i, f"{v:.3f}",
                color='#2D2D2D',
                va='center',
                fontsize=11,
                fontweight='bold')

    plt.title("Top Predictive Features for Promotion Decisions",
             fontsize=16, pad=25, fontweight='bold')
    plt.xlabel("Feature Importance Score", fontsize=14)
    plt.ylabel("", fontsize=12)
    plt.xlim(0, feat_imp.importance.max()*1.2)

    # Add insights box
    textstr = """Key Drivers:
1. Previous Year Performance (0.230)
2. Training Scores (0.180)
3. Organizational Tenure (0.150)
4. Awards Recognition (0.120)
5. Employee Age (0.080)"""
    plt.gcf().text(0.75, 0.68, textstr, fontsize=12,
                 bbox=dict(facecolor='#FFF0F0', alpha=0.9, edgecolor='#FF6B6B'))

    plt.grid(axis='x', alpha=0.4)
    sns.despine(left=True)
    plt.tight_layout()
    plt.show()

def plot_shap_values(model, X):
    """Enhanced SHAP plot with color variation"""
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)

    plt.figure(figsize=(14, 8))
    shap.summary_plot(
        shap_values,
        X,
        feature_names=[FEATURE_NAMES[i] for i in range(X.shape[1])],
        plot_type="bar",
        color=COLOR_PALETTE[0],
        max_display=15,
        show=False
    )

    # Styling enhancements
    plt.gcf().axes[0].set_xlabel("Impact on Model Output (SHAP Value)", fontsize=14)
    plt.title("Feature Impact Analysis - Promotion Predictions",
             fontsize=16, pad=20, fontweight='bold')
    plt.gca().set_facecolor('#FFF8F8')

    # Adjust color gradient
    for i, fc in enumerate(plt.gcf().axes[0].get_children()):
        if isinstance(fc, plt.Rectangle):
            fc.set_facecolor(COLOR_PALETTE[i%len(COLOR_PALETTE)])
            fc.set_edgecolor('#2D2D2D')

    plt.grid(axis='x', color='#AAAAAA', alpha=0.3)
    plt.tight_layout()
    plt.show()

# Example usage:
if __name__ == "__main__":
    !pip install shap

    # Replace with your actual data
    from sklearn.datasets import make_classification
    X, y = make_classification(n_features=20)

    # Get feature indices from your dataset
    feature_indices = np.array(list(FEATURE_NAMES.keys()))

    # Train model
    from xgboost import XGBClassifier
    model = XGBClassifier().fit(X, y)

    # Generate visualizations
    plot_feature_importance(model, feature_indices)
    plot_shap_values(model, X)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import shap
import pandas as pd

# Updated styling
plt.rcParams['figure.figsize'] = (10, 6)
sns.set_style("whitegrid")
COLOR_PALETTE = ["#FF6B6B", "#FF8E8E", "#FFAAAA", "#FFC3C3", "#FFD8D8"]

FEATURE_NAMES = {
    0: "Age",
    1: "Department",
    2: "Education Level",
    3: "Gender",
    4: "Region",
    5: "Previous Year Rating",
    6: "Number of Trainings",
    7: "KPIs Met",
    8: "Awards Won",
    9: "Average Training Score",
    10: "Length of Service",
    11: "Recruitment Channel",
    12: "Performance Bonus",
    13: "Peer Reviews",
    14: "Manager Approval",
    15: "Project Completion Rate",
    16: "Overtime Hours",
    17: "Salary Grade",
    18: "Team Size",
    19: "Tenure in Role"
}

def plot_feature_importance(model, feature_indices):
    importance = model.feature_importances_
    indices = np.argsort(importance)[-15:]

    feat_imp = pd.DataFrame({
        'feature': [FEATURE_NAMES[i] for i in feature_indices[indices]],
        'importance': importance[indices]
    }).sort_values('importance', ascending=True)

    colors = sns.color_palette(COLOR_PALETTE, n_colors=len(feat_imp))

    plt.figure(figsize=(10, 6))
    bar = sns.barplot(
        x='importance',
        y='feature',
        data=feat_imp,
        palette=colors,
        edgecolor='#2D2D2D',
        linewidth=0.5
    )

    for i, v in enumerate(feat_imp.importance):
        bar.text(v + 0.003, i, f"{v:.3f}",
                 color='#2D2D2D',
                 va='center',
                 fontsize=10,
                 fontweight='bold')

    plt.title("Top Predictive Features for Promotion Decisions", fontsize=14, pad=20, fontweight='bold')
    plt.xlabel("Feature Importance Score", fontsize=12)
    plt.ylabel("")
    plt.xlim(0, feat_imp.importance.max()*1.2)
    plt.grid(axis='x', alpha=0.3)
    sns.despine(left=True)
    plt.tight_layout()
    plt.show()

def plot_shap_values(model, X):
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)

    plt.figure(figsize=(10, 6))
    shap.summary_plot(
        shap_values,
        X,
        feature_names=[FEATURE_NAMES[i] for i in range(X.shape[1])],
        plot_type="bar",
        color=COLOR_PALETTE[0],
        max_display=15,
        show=False
    )

    ax = plt.gcf().axes[0]
    ax.set_facecolor("white")
    plt.title("Feature Impact Analysis - Promotion Predictions", fontsize=14, pad=15, fontweight='bold')
    plt.xlabel("Impact on Model Output (SHAP Value)", fontsize=12)
    plt.tight_layout()
    plt.show()

# Example usage
if __name__ == "__main__":
    from sklearn.datasets import make_classification
    from xgboost import XGBClassifier

    X, y = make_classification(n_features=20, random_state=42)
    feature_indices = np.array(list(FEATURE_NAMES.keys()))

    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss').fit(X, y)

    plot_feature_importance(model, feature_indices)
    plot_shap_values(model, X)
